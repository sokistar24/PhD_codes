# -*- coding: utf-8 -*-
"""comparison_ARQ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQaZJV3qR1BWLyOlT5qQq2_JAoGLwxNo
"""

import random
import numpy as np
import gym
from gym.spaces import Discrete

random.seed(42)
# Normal ARQ simulation
def normal_arq(num_bits, P, h1, h2):
    state = 'G'  # Start in the Good state
    transmitted_bits = []
    received_bits = []

    errors_in_good_state = 0
    errors_in_medium_state = 0
    errors_in_bad_state = 0
    total_retransmissions = 0

    for _ in range(num_bits):
        # Generate a random bit
        bit = random.choice([0, 1])
        transmitted_bits.append(bit)

        transmission_successful = False
        while not transmission_successful:
            # Simulate transmission based on the current state
            if state == 'G':
                received_bits.append(bit)
                transmission_successful = True
            elif state == 'M':
                if random.random() < h1:
                    received_bits.append(bit)
                    transmission_successful = True
                else:
                    received_bits.append(1 - bit)
                    errors_in_medium_state += 1
                    total_retransmissions += 1
            else:  # state == 'B'
                if random.random() < h2:
                    received_bits.append(bit)
                    transmission_successful = True
                else:
                    received_bits.append(1 - bit)
                    errors_in_bad_state += 1
                    total_retransmissions += 1

            # Transition to the next state
            state_probs = P[state]
            state = random.choices(['G', 'M', 'B'], weights=state_probs)[0]

    return transmitted_bits, received_bits, errors_in_good_state, errors_in_medium_state, errors_in_bad_state, total_retransmissions


# Q-learning
def q_learning(env, num_episodes, alpha, gamma, epsilon, max_steps_per_episode):
    num_states = env.observation_space.n
    num_actions = env.action_space.n
    q_table = np.zeros((num_states, num_actions))

    for episode in range(num_episodes):
        state = env.reset()
        done = False
        step = 0

        while not done and step < max_steps_per_episode:
            # Choose an action based on the Q-table and epsilon-greedy policy
            if random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()  # Explore
            else:
                action = np.argmax(q_table[state])  # Exploit

            # Take the action and observe the next state and reward
            next_state, reward, done = env.step(action)

            # Update the Q-table
            q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])

            # Update the current state
            state = next_state
            step += 1

    return q_table


# RL-based ARQ simulation
def rl_arq_simulation(env, q_table, num_bits):
    state = 'G'
    transmitted_bits = []
    received_bits = []
    total_retransmissions = 0

    for _ in range(num_bits):
        bit = random.choice([0, 1])
        transmitted_bits.append(bit)

        transmission_successful = False
        while not transmission_successful:
            action = np.argmax(q_table[env.state_to_index(state)])

            if state == 'G':
                received_bits.append(bit)
                transmission_successful = True
            elif state == 'M':
                if action == 0:  # Transmit
                    if random.random() < env.h1:
                        received_bits.append(bit)
                        transmission_successful = True
                    else:
                        received_bits.append(1 - bit)
                        total_retransmissions += 1
                elif action == 1:  # wait
                    if random.random() < env.h1:
                        received_bits.append(bit)
                        transmission_successful = True
                    else:
                        received_bits.append(1 - bit)
                        total_retransmissions += 1
            else:  # state == 'B'
                if action == 0:  # Transmit
                    if random.random() < env.h2:
                        received_bits.append(bit)
                        transmission_successful = True
                    else:
                        received_bits.append(1 - bit)
                        total_retransmissions += 1
                elif action == 1:  # wait
                    if random.random() < env.h2:
                        received_bits.append(bit)
                        transmission_successful = True
                    else:
                        received_bits.append(1 - bit)
                        #total_retransmissions += 1

            state_idx, reward, _ = env.step(action)
            state = env.state_space[state_idx]

    return transmitted_bits, received_bits, total_retransmissions


P = {
    'G': [0.7, 0.2, 0.1],
    'M': [0.5, 0.3, 0.2],
    'B': [0.6, 0.3, 0.1],
}
h1 = 0.6
h2 = 0.1
num_bits = 5000

# Normal ARQ
transmitted_bits, received_bits, errors_in_good_state, errors_in_medium_state, errors_in_bad_state, total_retransmissions = normal_arq(num_bits, P, h1, h2)
print("Normal ARQ - Total retransmissions:", total_retransmissions)

# RL-based ARQ


class ChannelEnvironment(gym.Env):
    def __init__(self, P, h1, h2):
        self.P = P
        self.h1 = h1
        self.h2 = h2
        self.state_space = ['G', 'M', 'B']
        self.action_space = gym.spaces.Discrete(2)
        self.observation_space = gym.spaces.Discrete(len(self.state_space))
        self.state = 'G'

    def step(self, action):
        bit = random.choice([0, 1])
        reward = 0


        if action == 1:  # wait {no-op}
            
            if self.state == 'G':
                reward = -0.8
            elif self.state == 'M':
                reward = -0.2
            elif self.state == 'B':
                reward = 0.8

        if action == 0: 
          # Add a small negative constant value for sending a packet
          reward -= 0.05
    
          if self.state == 'G':
              reward += 1
          elif self.state == 'M':

            if random.random() < self.h1 :
              received_bit = bit 
              reward += 1
            else:
              reward -= 0.5

          elif self.state == 'B':
              if random.random()<self.h2 :
                reward += 1
              else:
                reward -= 0.5  


            

        # Transition to the next state
        state_probs = self.P[self.state]
        next_state = random.choices(self.state_space, weights=state_probs)[0]
        self.state = next_state
        return self.state_space.index(next_state), reward, False


    def state_to_index(self, state):
        return self.state_space.index(state)

    def reset(self):
          self.state = 'G'
          return self.state_space.index(self.state)

env = ChannelEnvironment(P, h1, h2)
num_episodes = 10000
alpha = 0.1
gamma = 0.9
epsilon = 0.1
max_steps_per_episode = 100

q_table = q_learning(env, num_episodes, alpha, gamma, epsilon, max_steps_per_episode)

print(q_table)

transmitted_bits, received_bits, errors_in_good_state, errors_in_medium_state, errors_in_bad_state, total_retransmissions = normal_arq(num_bits, P, h1, h2)
print("Normal ARQ - Total retransmissions:", total_retransmissions)
transmitted_bits_rl, received_bits_rl, total_retransmissions_rl = rl_arq_simulation(env, q_table, num_bits)
print("RL-based ARQ - Total retransmissions:", total_retransmissions_rl)

